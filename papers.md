# CS231n部分论文

**机器学习须知** [a few useful things to know about machine learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)



**All-Vs-All SVM** [support vector machines for multi-class pattern recognition](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf)

**One-Vs-All SVM**  [in defense of one-vs-all classification](http://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf) 

**展示一些L2SVM比Softmax表现更出色的结果** [Deep Learning using Linear Support Vector Machines](https://arxiv.org/pdf/1306.0239.pdf)

**自动微分** [automatic differentiation in machine learning: a survey](https://arxiv.org/pdf/1502.05767.pdf)

**分层softmax**[Hierarchical Probabilistic Neural Network Language Model](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)



**神经网络深度** [Do deep nets really need to be deep](https://arxiv.org/pdf/1312.6184.pdf)

​                       [Fitnets: hints for thin deep nets](https://arxiv.org/pdf/1412.6550.pdf)

**1*1卷积**[network in network](https://arxiv.org/pdf/1312.4400.pdf)

**扩张卷积**[multi-scale context aggregation by dilated convolutions](https://arxiv.org/pdf/1511.07122.pdf)

**弃用汇聚层**[striving for simplicity: the all convolutional net](https://arxiv.org/pdf/1412.6806.pdf)

**LeNet** [Gradient-Based learning applied to document recognition](**LeNet**)

**AlexNet** [ImageNet classification with deep convolutional neural networks](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)  

**ZFNet**[Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)

**GoogleNet** [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261.pdf)

**VGGNet**[very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556.pdf)

**ResNet**[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)



**Batch normalization**[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)

**Layer normalization** [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

**Group normalization**[Group Normalization](https://arxiv.org/pdf/1803.08494.pdf)

**初始化**[all you need is a good init](https://arxiv.org/pdf/1511.06422.pdf)



**随机搜索**[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)

**浮点数须知**[What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)

**Dropout**[Dropout:  A simple way to prevent neural networks from overfitting](http://www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf)

**随机深度**[deep networks with stochastic depth](https://arxiv.org/pdf/1603.09382.pdf)







